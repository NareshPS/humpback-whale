{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from sys import stdout\n",
    "\n",
    "#Dataset imports\n",
    "import csv\n",
    "\n",
    "#Imports for image load/unload/process\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.applications import VGG16\n",
    "import cv2\n",
    "from skimage import transform\n",
    "from random import shuffle\n",
    "\n",
    "#Plotting libs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Graph keras model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Activation, MaxPool1D, Conv1D, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#Progress bars\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#Project constants\n",
    "from source.common import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000e88ab.jpg',\n",
       " '0001f9222.jpg',\n",
       " '00029d126.jpg',\n",
       " '00050a15a.jpg',\n",
       " '0005c1ef8.jpg',\n",
       " '0006e997e.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(source_path, files, batch_size, class_name_map, label_dict, num_classes):\n",
    "    #Image batch placeholder\n",
    "    x = None\n",
    "    \n",
    "    #Labels placeholder\n",
    "    y = None\n",
    "\n",
    "   # with tqdm(total = len(files), file=stdout) as progress_bar:\n",
    "    #    loaded = 0\n",
    "    while True:\n",
    "        shuffle(files)\n",
    "        for batch_files in batch(files, batch_size):\n",
    "            #Load images\n",
    "            x = load_dataset(source_path, batch_files)\n",
    "\n",
    "            #Normalize\n",
    "            x = np.array(x/255)\n",
    "\n",
    "            y = [class_name_map[label_dict[image]] for image in batch_files]\n",
    "            y = to_categorical(y, num_classes = num_classes)\n",
    "\n",
    "           # loaded += len(batch_files)\n",
    "           # progress_bar.set_description(\"Loaded {loaded}\".format(loaded = loaded))\n",
    "           # progress_bar.update(len(batch_files))\n",
    "\n",
    "            yield [x], y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"0000e88ab.jpg\", \"000a6daec.jpg\"]\n",
    "batch_size = 2\n",
    "for x, y in load_model_data(TRAIN_SET_LOC, files, batch_size, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES):\n",
    "    print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create label and class mapping for training set. ###\n",
    "\n",
    "#Load labels\n",
    "LABEL_DICT = {}\n",
    "\n",
    "with open(LABEL_FILE_LOC, 'r') as handle:\n",
    "    label_reader = csv.reader(handle)\n",
    "    next(label_reader, None)\n",
    "    \n",
    "    loaded_items = 0\n",
    "    for row in label_reader:\n",
    "        LABEL_DICT[row[0]] = row[1]\n",
    "    \n",
    "#Classes\n",
    "CLASS_NAMES = list(set(LABEL_DICT.values()))\n",
    "CLASS_NAME_MAP = {}\n",
    "\n",
    "class_idx = 0\n",
    "for class_name in CLASS_NAMES:\n",
    "    CLASS_NAME_MAP[class_name] = class_idx\n",
    "    class_idx += 1\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(\"Number of classses: {count}\".format(count = NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Preprocess train dataset ###\n",
    "files = list(LABEL_DICT.keys())\n",
    "\n",
    "with tqdm(total = len(files), file=stdout) as progress_bar:\n",
    "    preprocess_raw_dataset(RAW_TRAIN_SET_LOC, files, TRAIN_SET_LOC, IMG_SIZE, 256, progress_bar = progress_bar)\n",
    "\n",
    "\"\"\"\n",
    "train_raw_files = [\"0000e88ab.jpg\"]\n",
    "image = imread(locate_img(RAW_TRAIN_SET_LOC, \"0000e88ab.jpg\"))\n",
    "resized = load_dataset(RAW_TRAIN_SET_LOC, train_raw_files)\n",
    "print(resized[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(source_path, label_dict, num_files = 10):\n",
    "    files = list(label_dict.keys())[:num_files]\n",
    "\n",
    "    x = load_dataset(source_path, files)\n",
    "    x = to_grayscale(x)\n",
    "\n",
    "    y = [CLASS_NAME_MAP[LABEL_DICT[image]] for image in files]\n",
    "    y = to_categorical(y, num_classes = NUM_CLASSES)\n",
    "\n",
    "    #Print sample\n",
    "    plt.figure()\n",
    "\n",
    "    print(x[3])\n",
    "    plt.imshow(x[0], cmap='gray')\n",
    "    #plt.imshow(cvtColor((x[4]).astype('uint8'), COLOR_BGR2RGB)) #SAMPLE_IMG_ID\n",
    "\n",
    "    print(y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(LABEL_DICT.keys())[:10]\n",
    "\n",
    "x = load_dataset(TRAIN_SET_LOC, files)\n",
    "x = to_grayscale(x)\n",
    "\n",
    "y = [CLASS_NAME_MAP[LABEL_DICT[image]] for image in files]\n",
    "y = to_categorical(y, num_classes = NUM_CLASSES)\n",
    "\n",
    "#Print sample\n",
    "plt.figure()\n",
    "\n",
    "print(x[3])\n",
    "plt.imshow(x[0], cmap='gray')\n",
    "#plt.imshow(cvtColor((x[4]).astype('uint8'), COLOR_BGR2RGB)) #SAMPLE_IMG_ID\n",
    "\n",
    "print(y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create the model for gray-scale inputs ###\n",
    "model = Sequential()\n",
    "\n",
    "input_shape = IMG_SIZE\n",
    "\n",
    "model.add(Conv1D(32, kernel_size = 3, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv1D(32, kernel_size = 3, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(Conv1D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Print model summary\n",
    "#print(model.summary())\n",
    "\n",
    "#Training and validation sets\n",
    "files = list(LABEL_DICT.keys())[:2048]\n",
    "num_files = len(files)\n",
    "batch_size = 128\n",
    "validation_split = 0.2\n",
    "split_marker = int(num_files*(1 - validation_split))\n",
    "train_set = files[:split_marker]\n",
    "validation_set = files[split_marker:]\n",
    "\n",
    "#Train the model\n",
    "model.fit_generator(\n",
    "    load_model_data(TRAIN_SET_LOC, train_set, batch_size, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES),\n",
    "    steps_per_epoch = (len(train_set) + batch_size - 1)/batch_size,\n",
    "    epochs = 20,\n",
    "    validation_data=load_model_data(TRAIN_SET_LOC, validation_set, batch_size, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES),\n",
    "    validation_steps=(len(validation_set) + batch_size - 1)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the model ###\n",
    "model = Sequential()\n",
    "\n",
    "input_shape = IMG_SIZE\n",
    "\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid', input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(4, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NUM_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "#Train the model\n",
    "files = list(LABEL_DICT.keys())[:5096]\n",
    "batch_size = 16\n",
    "model.fit_generator(\n",
    "    load_model_data(TRAIN_SET_LOC, files, 16, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES),\n",
    "    epochs = 20, \n",
    "    steps_per_epoch = len(files)/batch_size + 1, \n",
    "    use_multiprocessing = True)\n",
    "\"\"\"\n",
    "for files in batch(list(LABEL_DICT.keys()), 256):\n",
    "    x, y = load_image_set(TRAIN_SET_LOC, files, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES)\n",
    "    model.fit(x, y, batch_size = 16, validation_split = 0.2, epochs=3)\n",
    "\"\"\"\n",
    "\n",
    "#VG(model_to_dot(model).create(prog='dot', format='s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scratch\n",
    "#img = imread(locate_train_img(\"0000e88ab.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
