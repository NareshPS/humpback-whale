{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from sys import platform, stdout\n",
    "\n",
    "#Dataset imports\n",
    "import csv\n",
    "\n",
    "#Imports for image load/unload/process\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
    "from keras.applications import VGG16\n",
    "import cv2\n",
    "from skimage import transform\n",
    "from random import shuffle\n",
    "\n",
    "#Plotting libs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Graph keras model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Activation, MaxPool1D, Conv1D, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#Progress bars\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants ###\n",
    "\n",
    "#Dataset constants for Windows\n",
    "if platform.startswith(\"win\"):\n",
    "    RAW_TRAIN_SET_LOC = \"..\\\\Humpback Whale\\\\dataset\\\\train\"\n",
    "    TRAIN_SET_LOC = \"..\\\\Humpback Whale\\\\dataset\\\\train_resized\"\n",
    "    PROCESSED_SET_LOC = \"..\\\\Humpback Whale\\\\dataset\\\\processed\"\n",
    "    RAW_TEST_SET_LOC = \"..\\\\Humpback Whale\\\\dataset\\\\test\"\n",
    "    TEST_SET_LOC = \"..\\\\Humpback Whale\\\\dataset\\\\test_resized\"\n",
    "    LABEL_FILE_LOC = \"..\\\\Humpback Whale\\\\dataset\\\\train.csv\"\n",
    "else:\n",
    "    RAW_TRAIN_SET_LOC = \"dataset/train\"\n",
    "    TRAIN_SET_LOC = \"dataset/train_resized\"\n",
    "    PROCESSED_SET_LOC = \"dataset/processed\"\n",
    "    RAW_TEST_SET_LOC = \"dataset/test\"\n",
    "    TEST_SET_LOC = \"dataset/test_resized\"\n",
    "    LABEL_FILE_LOC = \"dataset/train.csv\"\n",
    "\n",
    "#Dataset markers\n",
    "TRAIN_SET_SIZE = 20\n",
    "TEST_SET_SIZE = 10\n",
    "PREPROCESS_BATCH_START = 0\n",
    "PREPROCESS_BATCH_SIZE = 500\n",
    "\n",
    "#Image parameter constants\n",
    "IMG_SIZE = (400, 700)\n",
    "IMG_WIDTH = 700\n",
    "IMG_HEIGHT = 400\n",
    "IMG_EXTN = \"jpg\"\n",
    "\n",
    "#Sampling\n",
    "SAMPLE_IMG_ID = 5\n",
    "\n",
    "#Containers\n",
    "LABEL_DICT = {} \n",
    "X_TRAIN = []\n",
    "Y_TRAIN = []\n",
    "X_TEST = []\n",
    "Y_TEST = []\n",
    "CLASS_NAMES = []\n",
    "CLASSES = []\n",
    "NUM_CLASSES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility methods\n",
    "\n",
    "def locate_img(path, img_name):\n",
    "    return os.path.join(path, img_name)\n",
    "\n",
    "def load_dataset(dataset_path, files):\n",
    "    images = []\n",
    "    for file in files:\n",
    "        image = cv2.imread(locate_img(dataset_path, file), cv2.IMREAD_GRAYSCALE)\n",
    "        images.append(image)\n",
    "    \n",
    "    return np.asarray(images)\n",
    "\n",
    "def resize_images(images, target_size):\n",
    "    #impad() changes the image data from [0..255] range to [0..1]\n",
    "    resized_images = [impad(image, target_size) for image in images]\n",
    "    \n",
    "    return resized_images\n",
    "    \n",
    "def save_dataset(target_path, files, image_data):\n",
    "    for idx, file in enumerate(files):\n",
    "        data = image_data[idx]\n",
    "        file_path = locate_img(target_path, file)\n",
    "        cv2.imwrite(file_path, (data*255).astype('uint8'))\n",
    "\n",
    "def preprocess_raw_dataset(source_dataset_path, files, target_dataset_path, target_size, batch_size = PREPROCESS_BATCH_SIZE, progress_bar = None):\n",
    "    for batch_id, batch_files in enumerate(batch(files, batch_size)):\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.set_description(\"Processing batch: {batch_id}\".format(batch_id = batch_id))\n",
    "\n",
    "        #Load batch of images for processing.\n",
    "        batch_images = load_dataset(source_dataset_path, batch_files)\n",
    "\n",
    "        #Resize images to keep all images for a consistent size.\n",
    "        resized_batch = resize_images(batch_images, target_size)\n",
    "\n",
    "        #Save training images to be readily available to be trained.\n",
    "        save_dataset(target_dataset_path, batch_files, resized_batch)\n",
    "\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.update(len(batch_files))\n",
    "        \n",
    "def batch(iterable, batch_size = 1):\n",
    "    count = len(iterable)\n",
    "    for batch_idx in range(0, count, batch_size):\n",
    "        yield iterable[batch_idx:min(batch_idx + batch_size, count)]\n",
    "\n",
    "def impad(image, target_size):\n",
    "    return transform.resize(image, target_size, anti_aliasing = True)\n",
    "\n",
    "def load_model_data(source_path, files, batch_size, class_name_map, label_dict, num_classes):\n",
    "    #Image batch placeholder\n",
    "    x = None\n",
    "    \n",
    "    #Labels placeholder\n",
    "    y = None\n",
    "\n",
    "   # with tqdm(total = len(files), file=stdout) as progress_bar:\n",
    "    #    loaded = 0\n",
    "    while True:\n",
    "        shuffle(files)\n",
    "        for batch_files in batch(files, batch_size):\n",
    "            #Load images\n",
    "            x = load_dataset(source_path, batch_files)\n",
    "\n",
    "            #Normalize\n",
    "            x = np.array(x/255)\n",
    "\n",
    "            y = [class_name_map[label_dict[image]] for image in batch_files]\n",
    "            y = to_categorical(y, num_classes = num_classes)\n",
    "\n",
    "           # loaded += len(batch_files)\n",
    "           # progress_bar.set_description(\"Loaded {loaded}\".format(loaded = loaded))\n",
    "           # progress_bar.update(len(batch_files))\n",
    "\n",
    "            yield [x], y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"0000e88ab.jpg\", \"000a6daec.jpg\"]\n",
    "batch_size = 2\n",
    "for x, y in load_model_data(TRAIN_SET_LOC, files, batch_size, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES):\n",
    "    print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classses: 5005\n"
     ]
    }
   ],
   "source": [
    "### Create label and class mapping for training set. ###\n",
    "\n",
    "#Load labels\n",
    "LABEL_DICT = {}\n",
    "\n",
    "with open(LABEL_FILE_LOC, 'r') as handle:\n",
    "    label_reader = csv.reader(handle)\n",
    "    next(label_reader, None)\n",
    "    \n",
    "    loaded_items = 0\n",
    "    for row in label_reader:\n",
    "        LABEL_DICT[row[0]] = row[1]\n",
    "    \n",
    "#Classes\n",
    "CLASS_NAMES = list(set(LABEL_DICT.values()))\n",
    "CLASS_NAME_MAP = {}\n",
    "\n",
    "class_idx = 0\n",
    "for class_name in CLASS_NAMES:\n",
    "    CLASS_NAME_MAP[class_name] = class_idx\n",
    "    class_idx += 1\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(\"Number of classses: {count}\".format(count = NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac88440ee3c4b869c9c2443552950c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25361), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nares\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "### Preprocess train dataset ###\n",
    "files = list(LABEL_DICT.keys())\n",
    "\n",
    "with tqdm(total = len(files), file=stdout) as progress_bar:\n",
    "    preprocess_raw_dataset(RAW_TRAIN_SET_LOC, files, TRAIN_SET_LOC, IMG_SIZE, 256, progress_bar = progress_bar)\n",
    "\n",
    "\"\"\"\n",
    "train_raw_files = [\"0000e88ab.jpg\"]\n",
    "image = imread(locate_img(RAW_TRAIN_SET_LOC, \"0000e88ab.jpg\"))\n",
    "resized = load_dataset(RAW_TRAIN_SET_LOC, train_raw_files)\n",
    "print(resized[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(source_path, label_dict, num_files = 10):\n",
    "    files = list(label_dict.keys())[:num_files]\n",
    "\n",
    "    x = load_dataset(source_path, files)\n",
    "    x = to_grayscale(x)\n",
    "\n",
    "    y = [CLASS_NAME_MAP[LABEL_DICT[image]] for image in files]\n",
    "    y = to_categorical(y, num_classes = NUM_CLASSES)\n",
    "\n",
    "    #Print sample\n",
    "    plt.figure()\n",
    "\n",
    "    print(x[3])\n",
    "    plt.imshow(x[0], cmap='gray')\n",
    "    #plt.imshow(cvtColor((x[4]).astype('uint8'), COLOR_BGR2RGB)) #SAMPLE_IMG_ID\n",
    "\n",
    "    print(y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(LABEL_DICT.keys())[:10]\n",
    "\n",
    "x = load_dataset(TRAIN_SET_LOC, files)\n",
    "x = to_grayscale(x)\n",
    "\n",
    "y = [CLASS_NAME_MAP[LABEL_DICT[image]] for image in files]\n",
    "y = to_categorical(y, num_classes = NUM_CLASSES)\n",
    "\n",
    "#Print sample\n",
    "plt.figure()\n",
    "\n",
    "print(x[3])\n",
    "plt.imshow(x[0], cmap='gray')\n",
    "#plt.imshow(cvtColor((x[4]).astype('uint8'), COLOR_BGR2RGB)) #SAMPLE_IMG_ID\n",
    "\n",
    "print(y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/13 [==============================] - 56s 4s/step - loss: 9.0712 - acc: 0.3145 - val_loss: 9.5271 - val_acc: 0.4089\n",
      "Epoch 2/20\n",
      "14/13 [==============================] - 48s 3s/step - loss: 9.5532 - acc: 0.4073 - val_loss: 9.6169 - val_acc: 0.4033\n",
      "Epoch 3/20\n",
      "14/13 [==============================] - 40s 3s/step - loss: 9.7482 - acc: 0.3952 - val_loss: 9.3473 - val_acc: 0.4201\n",
      "Epoch 4/20\n",
      "14/13 [==============================] - 38s 3s/step - loss: 9.5480 - acc: 0.4076 - val_loss: 9.3899 - val_acc: 0.4174\n",
      "Epoch 5/20\n",
      "14/13 [==============================] - 40s 3s/step - loss: 9.9724 - acc: 0.3813 - val_loss: 9.0777 - val_acc: 0.4368\n",
      "Epoch 6/20\n",
      "14/13 [==============================] - 40s 3s/step - loss: 9.4871 - acc: 0.4114 - val_loss: 9.7667 - val_acc: 0.3941\n",
      "Epoch 7/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.4897 - acc: 0.4112 - val_loss: 9.5570 - val_acc: 0.4071\n",
      "Epoch 8/20\n",
      "14/13 [==============================] - 38s 3s/step - loss: 9.9000 - acc: 0.3858 - val_loss: 9.5008 - val_acc: 0.4106\n",
      "Epoch 9/20\n",
      "14/13 [==============================] - 40s 3s/step - loss: 9.3281 - acc: 0.4213 - val_loss: 9.2574 - val_acc: 0.4257\n",
      "Epoch 10/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.8956 - acc: 0.3861 - val_loss: 9.7667 - val_acc: 0.3941\n",
      "Epoch 11/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.6041 - acc: 0.4041 - val_loss: 9.4671 - val_acc: 0.4126\n",
      "Epoch 12/20\n",
      "14/13 [==============================] - 38s 3s/step - loss: 9.8198 - acc: 0.3908 - val_loss: 9.3899 - val_acc: 0.4174\n",
      "Epoch 13/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.5400 - acc: 0.4081 - val_loss: 9.3173 - val_acc: 0.4219\n",
      "Epoch 14/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.7534 - acc: 0.3949 - val_loss: 9.6769 - val_acc: 0.3996\n",
      "Epoch 15/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.5823 - acc: 0.4055 - val_loss: 9.2874 - val_acc: 0.4238\n",
      "Epoch 16/20\n",
      "14/13 [==============================] - 38s 3s/step - loss: 9.7525 - acc: 0.3949 - val_loss: 9.6487 - val_acc: 0.4014\n",
      "Epoch 17/20\n",
      "14/13 [==============================] - 40s 3s/step - loss: 9.5171 - acc: 0.4095 - val_loss: 9.4072 - val_acc: 0.4164\n",
      "Epoch 18/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.6603 - acc: 0.4007 - val_loss: 9.7368 - val_acc: 0.3959\n",
      "Epoch 19/20\n",
      "14/13 [==============================] - 39s 3s/step - loss: 9.7821 - acc: 0.3931 - val_loss: 9.3773 - val_acc: 0.4182\n",
      "Epoch 20/20\n",
      "14/13 [==============================] - 38s 3s/step - loss: 9.7015 - acc: 0.3981 - val_loss: 9.3529 - val_acc: 0.4197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f48021b278>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the model for gray-scale inputs ###\n",
    "model = Sequential()\n",
    "\n",
    "input_shape = IMG_SIZE\n",
    "\n",
    "model.add(Conv1D(32, kernel_size = 3, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv1D(32, kernel_size = 3, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(Conv1D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Print model summary\n",
    "#print(model.summary())\n",
    "\n",
    "#Training and validation sets\n",
    "files = list(LABEL_DICT.keys())[:2048]\n",
    "num_files = len(files)\n",
    "batch_size = 128\n",
    "validation_split = 0.2\n",
    "split_marker = int(num_files*(1 - validation_split))\n",
    "train_set = files[:split_marker]\n",
    "validation_set = files[split_marker:]\n",
    "\n",
    "#Train the model\n",
    "model.fit_generator(\n",
    "    load_model_data(TRAIN_SET_LOC, train_set, batch_size, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES),\n",
    "    steps_per_epoch = (len(train_set) + batch_size - 1)/batch_size,\n",
    "    epochs = 20,\n",
    "    validation_data=load_model_data(TRAIN_SET_LOC, validation_set, batch_size, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES),\n",
    "    validation_steps=(len(validation_set) + batch_size - 1)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the model ###\n",
    "model = Sequential()\n",
    "\n",
    "input_shape = IMG_SIZE\n",
    "\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid', input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(4, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NUM_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "#Train the model\n",
    "files = list(LABEL_DICT.keys())[:5096]\n",
    "batch_size = 16\n",
    "model.fit_generator(\n",
    "    load_model_data(TRAIN_SET_LOC, files, 16, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES),\n",
    "    epochs = 20, \n",
    "    steps_per_epoch = len(files)/batch_size + 1, \n",
    "    use_multiprocessing = True)\n",
    "\"\"\"\n",
    "for files in batch(list(LABEL_DICT.keys()), 256):\n",
    "    x, y = load_image_set(TRAIN_SET_LOC, files, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES)\n",
    "    model.fit(x, y, batch_size = 16, validation_split = 0.2, epochs=3)\n",
    "\"\"\"\n",
    "\n",
    "#VG(model_to_dot(model).create(prog='dot', format='s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scratch\n",
    "#img = imread(locate_train_img(\"0000e88ab.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
