{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "from sys import argv, stdout\n",
    "\n",
    "#To save training data\n",
    "from pickle import dump as pickle_dump\n",
    "\n",
    "#Data adjustment\n",
    "import numpy as np\n",
    "\n",
    "#Local imports\n",
    "from common import constants\n",
    "from models import cnn_gray_model_1\n",
    "from model_utils import get_input_labels, get_label_ids, model_fit, load_training_batch, load_training\n",
    "from utils import list_files\n",
    "\n",
    "batch_size = 2\n",
    "n_images = 3\n",
    "n_epochs = 30\n",
    "o_model = \"model_1\"\n",
    "l_rate = 0.001\n",
    "\n",
    "dataset = \"train\"\n",
    "validation_split = 0.2\n",
    "input_shape = constants.IMG_SHAPE\n",
    "source_loc = constants.PROCESSED_DATASET_MAPPINGS[dataset]\n",
    "\n",
    "input_labels = get_input_labels()\n",
    "label_ids = get_label_ids()\n",
    "n_classes = len(label_ids)\n",
    "input_set = list_files(source_loc, n_images)\n",
    "\n",
    "x, y = load_training(source_loc, input_set, input_labels, label_ids)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_image(tensor):\n",
    "    \"\"\"\n",
    "    Convert an numpy representation image to Image protobuf.\n",
    "    Copied from https://github.com/lanpa/tensorboard-pytorch/\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    height, width, channel = tensor.shape\n",
    "    image = Image.fromarray(tensor)\n",
    "    import io\n",
    "    output = io.BytesIO()\n",
    "    image.save(output, format='PNG')\n",
    "    image_string = output.getvalue()\n",
    "    output.close()\n",
    "    return tf.Summary.Image(height=height,\n",
    "                         width=width,\n",
    "                         colorspace=channel,\n",
    "                         encoded_image_string=image_string)\n",
    "\n",
    "class TensorBoardImage(keras.callbacks.Callback):\n",
    "    def __init__(self, tag):\n",
    "        super().__init__() \n",
    "        self.tag = tag\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Load image\n",
    "        img = data.astronaut()\n",
    "        # Do something to the image\n",
    "        img = (255 * skimage.util.random_noise(img)).astype('uint8')\n",
    "\n",
    "        image = make_image(img)\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=self.tag, image=image)])\n",
    "        writer = tf.summary.FileWriter('./logs')\n",
    "        writer.add_summary(summary, epoch)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Command line parameters\n",
    "from sys import argv\n",
    "\n",
    "#Load training history\n",
    "from pickle import load as pickle_load\n",
    "\n",
    "#Load keras model\n",
    "from keras.models import load_model\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "model_name = \"cnn_gray_model_1\"\n",
    "history_file = \"{model_name}.hist\".format(model_name = model_name)\n",
    "model_file = \"{model_name}.h5\".format(model_name = model_name)\n",
    "\n",
    "print(\"Using model: {model_file} history: {history_file}\".format(model_file = model_file, history_file = history_file))\n",
    "\n",
    "model = load_model(model_file)\n",
    "\n",
    "history = None\n",
    "with open(history_file, 'rb') as handle:\n",
    "    history = pickle_load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from visualization import HistoryInsights\n",
    "\n",
    "insights = HistoryInsights(history)\n",
    "insights.accuracy()\n",
    "insights.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_1\"\n",
    "history_file = \"{model_name}.hist\".format(model_name = model_name)\n",
    "model_file = \"{model_name}.h5\".format(model_name = model_name)\n",
    "from keras.models import load_model\n",
    "model = load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class ModelCnnSummary:\n",
    "    _visual_layer_prefix = 'conv'\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def summary(self):\n",
    "        #Useful objects\n",
    "        model = self._model\n",
    "        layers = model.layers\n",
    "\n",
    "        print(\"Layers: {}\".format(len(layers)))\n",
    "        \n",
    "        print(\"\\nNames and shapes.\")\n",
    "        for layer in layers:\n",
    "            print(\"({}, {})\".format(layer.name, layer.output_shape))\n",
    "        \n",
    "        print(\"\\nNames and weights.\")\n",
    "        for layer in layers:\n",
    "            if layer.name.startswith(self._visual_layer_prefix):\n",
    "                weights = layer.get_weights()[0]\n",
    "                print(weights.shape)\n",
    "                plt.figure()\n",
    "                plt.imshow(weights[8,:, :])\n",
    "\n",
    "model = load_model(model_file)       \n",
    "summary = ModelCnnSummary(model)\n",
    "summary.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4 validation set: 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 133, 233, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 77, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 12, 64)         18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5005)              1286285   \n",
      "=================================================================\n",
      "Total params: 1,384,301\n",
      "Trainable params: 1,384,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f007416a00d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\Work\\Scratch\\humpback-whale\\model_utils.py\u001b[0m in \u001b[0;36mmodel_fit\u001b[1;34m(model, source_loc, train_set, validation_set, input_labels, label_ids, batch_size, n_epochs)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \"\"\"\n\u001b[0;32m    154\u001b[0m     \u001b[1;31m#Generate validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;31m#Training summary collection callback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\Work\\Scratch\\humpback-whale\\model_utils.py\u001b[0m in \u001b[0;36mload_training_data\u001b[1;34m(source_loc, input_set, input_labels, label_ids)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\Work\\Scratch\\humpback-whale\\model_utils.py\u001b[0m in \u001b[0;36mload_training_batch\u001b[1;34m(requestor, source_loc, img_files, batch_size, input_labels, label_ids)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_img_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mload_images_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;31m#Normalize image data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\Work\\Scratch\\humpback-whale\\utils.py\u001b[0m in \u001b[0;36mload_images_batch\u001b[1;34m(source_loc, img_files, batch_size, progress_bar)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#Local imports\n",
    "from common import constants\n",
    "from models import cnn_model_2d_1\n",
    "from model_utils import get_input_labels, get_label_ids, model_fit\n",
    "from utils import list_files, split_dataset\n",
    "\n",
    "dataset = \"train\"\n",
    "v_split = 0.2\n",
    "n_images = 6\n",
    "batch_size = 2\n",
    "l_rate = 0.001\n",
    "n_epochs = 1\n",
    "input_shape = constants.IMG_SHAPE\n",
    "source_loc = constants.PROCESSED_DATASET_MAPPINGS[dataset]\n",
    "\n",
    "#Input data parameters\n",
    "input_labels = get_input_labels()\n",
    "label_ids = get_label_ids()\n",
    "n_classes = len(label_ids)\n",
    "input_set = list_files(source_loc, n_images)\n",
    "n_images = len(input_set)\n",
    "\n",
    "#Training and validation sets\n",
    "train_set, validation_set = split_dataset(input_set, v_split)\n",
    "print(\"Training set: {t_size} validation set: {v_size}\".format(t_size = len(train_set), v_size = len(validation_set)))\n",
    "\n",
    "model = cnn_model_2d_1(input_shape, n_classes, l_rate)\n",
    "model.summary()\n",
    "\n",
    "#Train the model\n",
    "history = model_fit(model, source_loc, train_set, validation_set, input_labels, label_ids, batch_size, n_epochs)\n",
    "\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the model ###\n",
    "model = Sequential()\n",
    "\n",
    "input_shape = IMG_SIZE\n",
    "\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid', input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Conv2D(4, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D((5, 5), (2, 2), 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NUM_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "#Train the model\n",
    "files = list(LABEL_DICT.keys())[:5096]\n",
    "batch_size = 16\n",
    "model.fit_generator(\n",
    "    load_model_data(TRAIN_SET_LOC, files, 16, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES),\n",
    "    epochs = 20, \n",
    "    steps_per_epoch = len(files)/batch_size + 1, \n",
    "    use_multiprocessing = True)\n",
    "\"\"\"\n",
    "for files in batch(list(LABEL_DICT.keys()), 256):\n",
    "    x, y = load_image_set(TRAIN_SET_LOC, files, CLASS_NAME_MAP, LABEL_DICT, NUM_CLASSES)\n",
    "    model.fit(x, y, batch_size = 16, validation_split = 0.2, epochs=3)\n",
    "\"\"\"\n",
    "\n",
    "#VG(model_to_dot(model).create(prog='dot', format='s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scratch\n",
    "#img = imread(locate_train_img(\"0000e88ab.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
